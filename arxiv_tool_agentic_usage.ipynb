{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-11-06T11:48:16.149743Z",
     "start_time": "2025-11-06T11:48:13.545536Z"
    }
   },
   "source": [
    "from parse_config import get_api_key\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain.agents import create_agent\n",
    "from arxiv_api_tool import get_arxiv_papers\n",
    "import os\n",
    "\n",
    "api_key = get_api_key()\n",
    "\n",
    "if \"GOOGLE_API_KEY\" not in os.environ:\n",
    "    os.environ[\"GOOGLE_API_KEY\"] = api_key\n",
    "\n",
    "llm = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-2.5-flash\",\n",
    "    temperature=0,\n",
    "    max_tokens=None,\n",
    "    timeout=None,\n",
    "    max_retries=2\n",
    ")"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-06T11:48:19.568265Z",
     "start_time": "2025-11-06T11:48:19.475215Z"
    }
   },
   "cell_type": "code",
   "source": "agent = create_agent(model=llm, tools = [get_arxiv_papers])",
   "id": "f9825f36f158dbf9",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-06T11:48:29.052458Z",
     "start_time": "2025-11-06T11:48:21.288794Z"
    }
   },
   "cell_type": "code",
   "source": [
    "query = \"Any interesting arxiv papers about machine learning published in the last month?\"\n",
    "result = agent.invoke(\n",
    "    {\"messages\": [{\"role\": \"user\", \"content\": query}]}\n",
    ")"
   ],
   "id": "fb9e237ddf8a1313",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model called get_arxiv_papers with args: query=machine learning, max_results=5, start=0, last_month=True\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-06T11:48:30.252864Z",
     "start_time": "2025-11-06T11:48:30.246518Z"
    }
   },
   "cell_type": "code",
   "source": "result",
   "id": "534f3473e975112e",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'messages': [HumanMessage(content='Any interesting arxiv papers about machine learning published in the last month?', additional_kwargs={}, response_metadata={}, id='fa9fe71a-1774-40aa-9de6-69c3d65403e0'),\n",
       "  AIMessage(content='', additional_kwargs={'function_call': {'name': 'get_arxiv_papers', 'arguments': '{\"last_month\": true, \"query\": \"machine learning\", \"max_results\": 5}'}}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.5-flash', 'safety_ratings': [], 'grounding_metadata': {}, 'model_provider': 'google_genai'}, id='lc_run--2e8efb98-8cbd-47fa-b98e-31d1bc4cc69c-0', tool_calls=[{'name': 'get_arxiv_papers', 'args': {'last_month': True, 'query': 'machine learning', 'max_results': 5}, 'id': 'd6268ca8-51fc-43f1-b5ec-7662bc083e3b', 'type': 'tool_call'}], usage_metadata={'input_tokens': 204, 'output_tokens': 188, 'total_tokens': 392, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 157}}),\n",
       "  ToolMessage(content=\"id: http://arxiv.org/abs/2510.14837v1 \\nTitle: Reinforcement Learning with Stochastic Reward Machines \\nAuthors: ['Jan Corazza', 'Ivan Gavran', 'Daniel Neider'] \\nPublished: 2025-10-16 16:12:04+00:00 \\nSummary  Reward machines are an established tool for dealing with reinforcementlearning problems in which rewards are sparse and depend on complex sequencesof actions. However, existing algorithms for learning reward machines assume anoverly idealized setting where rewards have to be free of noise. To overcomethis practical limitation, we introduce a novel type of reward machines, calledstochastic reward machines, and an algorithm for learning them. Our algorithm,based on constraint solving, learns minimal stochastic reward machines from theexplorations of a reinforcement learning agent. This algorithm can easily bepaired with existing reinforcement learning algorithms for reward machines andguarantees to converge to an optimal policy in the limit. We demonstrate theeffectiveness of our algorithm in two case studies and show that it outperformsboth existing methods and a naive approach for handling noisy reward functions. \\nPDF link: http://arxiv.org/pdf/2510.14837v1 \\n\\n --- \\nid: http://arxiv.org/abs/2510.26100v1 \\nTitle: Applications of Machine Learning in Polymer Materials: Property Prediction, Material Design, and Systematic Processes \\nAuthors: ['Hongtao Guo Shuai Li Shu Li'] \\nPublished: 2025-10-30 03:26:04+00:00 \\nSummary  This paper systematically reviews the research progress and applicationprospects of machine learning technologies in the field of polymer materials.Currently, machine learning methods are developing rapidly in polymer materialresearch; although they have significantly accelerated material prediction anddesign, their complexity has also caused difficulties in understanding andapplication for researchers in traditional fields. In response to the aboveissues, this paper first analyzes the inherent challenges in the research anddevelopment of polymer materials, including structural complexity and thelimitations of traditional trial-and-error methods. To address these problems,it focuses on introducing key basic technologies such as molecular descriptorsand feature representation, data standardization and cleaning, and records anumber of high-quality polymer databases. Subsequently, it elaborates on thekey role of machine learning in polymer property prediction and materialdesign, covering the specific applications of algorithms such as traditionalmachine learning, deep learning, and transfer learning; further, it deeplyexpounds on data-driven design strategies, such as reverse design,high-throughput virtual screening, and multi-objective optimization. The paperalso systematically introduces the complete process of constructinghigh-reliability machine learning models and summarizes effective experimentalverification, model evaluation, and optimization methods. Finally, itsummarizes the current technical challenges in research, such as data qualityand model generalization ability, and looks forward to future developmenttrends including multi-scale modeling, physics-informed machine learning,standardized data sharing, and interpretable machine learning. \\nPDF link: http://arxiv.org/pdf/2510.26100v1 \\n\\n --- \\nid: http://arxiv.org/abs/2510.15422v1 \\nTitle: Information Theory in Open-world Machine Learning Foundations, Frameworks, and Future Direction \\nAuthors: ['Lin Wang'] \\nPublished: 2025-10-17 08:20:56+00:00 \\nSummary  Open world Machine Learning (OWML) aims to develop intelligent systemscapable of recognizing known categories, rejecting unknown samples, andcontinually learning from novel information. Despite significant progress inopen set recognition, novelty detection, and continual learning, the fieldstill lacks a unified theoretical foundation that can quantify uncertainty,characterize information transfer, and explain learning adaptability indynamic, nonstationary environments. This paper presents a comprehensive reviewof information theoretic approaches in open world machine learning, emphasizinghow core concepts such as entropy, mutual information, and Kullback Leiblerdivergence provide a mathematical language for describing knowledgeacquisition, uncertainty suppression, and risk control under open worldconditions. We synthesize recent studies into three major research axes:information theoretic open set recognition enabling safe rejection of unknowns,information driven novelty discovery guiding new concept formation, andinformation retentive continual learning ensuring stable long term adaptation.Furthermore, we discuss theoretical connections between information theory andprovable learning frameworks, including PAC Bayes bounds, open-space risktheory, and causal information flow, to establish a pathway toward provable andtrustworthy open world intelligence. Finally, the review identifies key openproblems and future research directions, such as the quantification ofinformation risk, development of dynamic mutual information bounds, multimodalinformation fusion, and integration of information theory with causal reasoningand world model learning. \\nPDF link: http://arxiv.org/pdf/2510.15422v1 \\n\\n --- \\nid: http://arxiv.org/abs/2510.23534v2 \\nTitle: Direct Debiased Machine Learning via Bregman Divergence Minimization \\nAuthors: ['Masahiro Kato'] \\nPublished: 2025-10-27 17:10:43+00:00 \\nSummary  We develop a direct debiased machine learning framework comprising Neymantargeted estimation and generalized Riesz regression. Our framework unifiesRiesz regression for automatic debiased machine learning, covariate balancing,targeted maximum likelihood estimation (TMLE), and density-ratio estimation. Inmany problems involving causal effects or structural models, the parameters ofinterest depend on regression functions. Plugging regression functionsestimated by machine learning methods into the identifying equations can yieldpoor performance because of first-stage bias. To reduce such bias, debiasedmachine learning employs Neyman orthogonal estimating equations. Debiasedmachine learning typically requires estimation of the Riesz representer and theregression function. For this problem, we develop a direct debiased machinelearning framework with an end-to-end algorithm. We formulate estimation of thenuisance parameters, the regression function and the Riesz representer, asminimizing the discrepancy between Neyman orthogonal scores computed with knownand unknown nuisance parameters, which we refer to as Neyman targetedestimation. Neyman targeted estimation includes Riesz representer estimation,and we measure discrepancies using the Bregman divergence. The Bregmandivergence encompasses various loss functions as special cases, where thesquared loss yields Riesz regression and the Kullback-Leibler divergence yieldsentropy balancing. We refer to this Riesz representer estimation as generalizedRiesz regression. Neyman targeted estimation also yields TMLE as a special casefor regression function estimation. Furthermore, for specific pairs of modelsand Riesz representer estimation methods, we can automatically obtain thecovariate balancing property without explicitly solving the covariate balancingobjective. \\nPDF link: http://arxiv.org/pdf/2510.23534v2 \\n\\n --- \\nid: http://arxiv.org/abs/2510.07569v2 \\nTitle: Automated Machine Learning for Unsupervised Tabular Tasks \\nAuthors: ['Prabhant Singh', 'Pieter Gijsbers', 'Elif Ceren Gok Yildirim', 'Murat Onur Yildirim', 'Joaquin Vanschoren'] \\nPublished: 2025-10-08 21:31:22+00:00 \\nSummary  In this work, we present LOTUS (Learning to Learn with Optimal Transport forUnsupervised Scenarios), a simple yet effective method to perform modelselection for multiple unsupervised machine learning(ML) tasks such as outlierdetection and clustering. Our intuition behind this work is that a machinelearning pipeline will perform well in a new dataset if it previously workedwell on datasets with a similar underlying data distribution. We use OptimalTransport distances to find this similarity between unlabeled tabular datasetsand recommend machine learning pipelines with one unified single method on twodownstream unsupervised tasks: outlier detection and clustering. We present theeffectiveness of our approach with experiments against strong baselines andshow that LOTUS is a very promising first step toward model selection formultiple unsupervised ML tasks. \\nPDF link: http://arxiv.org/pdf/2510.07569v2 \\n\", name='get_arxiv_papers', id='11fd93ad-12a6-4eab-b3fa-d7ba38d806ef', tool_call_id='d6268ca8-51fc-43f1-b5ec-7662bc083e3b'),\n",
       "  AIMessage(content='Here are 5 interesting arXiv papers about machine learning published in the last month:\\n\\n1.  **Reinforcement Learning with Stochastic Reward Machines** by Jan Corazza, Ivan Gavran, and Daniel Neider (2025-10-16): This paper introduces stochastic reward machines to handle noisy rewards in reinforcement learning, offering an algorithm that learns minimal stochastic reward machines and guarantees convergence to an optimal policy.\\n\\n2.  **Applications of Machine Learning in Polymer Materials: Property Prediction, Material Design, and Systematic Processes** by Hongtao Guo Shuai Li Shu Li (2025-10-30): This review systematically covers the use of machine learning in polymer materials for property prediction and material design, discussing key technologies, databases, algorithms, and future trends.\\n\\n3.  **Information Theory in Open-world Machine Learning Foundations, Frameworks, and Future Direction** by Lin Wang (2025-10-17): This paper reviews information-theoretic approaches in open-world machine learning, focusing on how concepts like entropy and mutual information can quantify uncertainty and characterize information transfer in dynamic environments.\\n\\n4.  **Direct Debiased Machine Learning via Bregman Divergence Minimization** by Masahiro Kato (2025-10-27): This work develops a direct debiased machine learning framework that unifies various methods like Riesz regression and targeted maximum likelihood estimation, aiming to reduce bias in causal effect and structural model estimations.\\n\\n5.  **Automated Machine Learning for Unsupervised Tabular Tasks** by Prabhant Singh, Pieter Gijsbers, Elif Ceren Gok Yildirim, Murat Onur Yildirim, and Joaquin Vanschoren (2025-10-08): This paper presents LOTUS, a method for model selection in unsupervised machine learning tasks like outlier detection and clustering, by using Optimal Transport distances to find similarities between unlabeled tabular datasets.', additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.5-flash', 'safety_ratings': [], 'grounding_metadata': {}, 'model_provider': 'google_genai'}, id='lc_run--d0aafd17-1f9f-4a65-993e-6ddecf3e4d16-0', usage_metadata={'input_tokens': 2053, 'output_tokens': 397, 'total_tokens': 2450, 'input_token_details': {'cache_read': 0}})]}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-06T11:59:00.630376Z",
     "start_time": "2025-11-06T11:59:00.624849Z"
    }
   },
   "cell_type": "code",
   "source": "print(result[\"messages\"][-1].content)",
   "id": "cfe78b9a8c1f9bf8",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here are 5 interesting arXiv papers about machine learning published in the last month:\n",
      "\n",
      "1.  **Reinforcement Learning with Stochastic Reward Machines** by Jan Corazza, Ivan Gavran, and Daniel Neider (2025-10-16): This paper introduces stochastic reward machines to handle noisy rewards in reinforcement learning, offering an algorithm that learns minimal stochastic reward machines and guarantees convergence to an optimal policy.\n",
      "\n",
      "2.  **Applications of Machine Learning in Polymer Materials: Property Prediction, Material Design, and Systematic Processes** by Hongtao Guo Shuai Li Shu Li (2025-10-30): This review systematically covers the use of machine learning in polymer materials for property prediction and material design, discussing key technologies, databases, algorithms, and future trends.\n",
      "\n",
      "3.  **Information Theory in Open-world Machine Learning Foundations, Frameworks, and Future Direction** by Lin Wang (2025-10-17): This paper reviews information-theoretic approaches in open-world machine learning, focusing on how concepts like entropy and mutual information can quantify uncertainty and characterize information transfer in dynamic environments.\n",
      "\n",
      "4.  **Direct Debiased Machine Learning via Bregman Divergence Minimization** by Masahiro Kato (2025-10-27): This work develops a direct debiased machine learning framework that unifies various methods like Riesz regression and targeted maximum likelihood estimation, aiming to reduce bias in causal effect and structural model estimations.\n",
      "\n",
      "5.  **Automated Machine Learning for Unsupervised Tabular Tasks** by Prabhant Singh, Pieter Gijsbers, Elif Ceren Gok Yildirim, Murat Onur Yildirim, and Joaquin Vanschoren (2025-10-08): This paper presents LOTUS, a method for model selection in unsupervised machine learning tasks like outlier detection and clustering, by using Optimal Transport distances to find similarities between unlabeled tabular datasets.\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "79574ce3838ad06"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
